{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<h1 align=\"center\"><font size=\"5\">RECOMMENDATION SYSTEM WITH A RESTRICTED BOLTZMANN MACHINE</font></h1>"}, {"metadata": {}, "cell_type": "markdown", "source": "Welcome to the <b>Recommendation System with a Restricted Boltzmann Machine</b> notebook. In this notebook, we study and go over the usage of a Restricted Boltzmann Machine (RBM) in a Collaborative Filtering based recommendation system. This system is an algorithm that recommends items by trying to find users that are similar to each other based on their item ratings. By the end of this notebook, you should have a deeper understanding of how Restricted Boltzmann Machines are applied, and how to build one using TensorFlow."}, {"metadata": {}, "cell_type": "markdown", "source": "<h2>Table of Contents</h2>\n\n<ol>\n    <li><a href=\"#ref1\">Acquiring the Data</a></li>\n    <li><a href=\"#ref2\">Loading in the Data</a></li>\n    <li><a href=\"#ref3\">The Restricted Boltzmann Machine model</a></li>\n    <li><a href=\"#ref4\">Setting the Model's Parameters</a></li>\n    <li><a href=\"#ref5\">Recommendation</a></li>\n</ol>\n<br>\n<br>\n<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"ref1\"></a>\n<h2>Acquiring the Data</h2>"}, {"metadata": {}, "cell_type": "markdown", "source": "To start, we need to download the data we are going to use for our system. The datasets we are going to use were acquired by <a href=\"http://grouplens.org/datasets/movielens/\">GroupLens</a> and contain movies, users and movie ratings by these users.\n\nAfter downloading the data, we will extract the datasets to a directory that is easily accessible."}, {"metadata": {}, "cell_type": "code", "source": "!wget -c https://raw.githubusercontent.com/IBM/dl-learning-path-assets/main/unsupervised-deeplearning/data/ml-1m.zip -O moviedataset.zip\n!unzip -o moviedataset.zip", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "With the datasets in place, let's now import the necessary libraries. We will be using <a href=\"https://www.tensorflow.org/\">Tensorflow</a> and <a href=\"http://www.numpy.org/\">Numpy</a> together to model and initialize our Restricted Boltzmann Machine and <a href=\"http://pandas.pydata.org/pandas-docs/stable/\">Pandas</a> to manipulate our datasets. To import these libraries, run the code cell below."}, {"metadata": {}, "cell_type": "code", "source": "#Tensorflow library. Used to implement machine learning models\nimport tensorflow as tf\n#Numpy contains helpful functions for efficient mathematical calculations\nimport numpy as np\n#Dataframe manipulation library\nimport pandas as pd\n#Graph plotting library\nimport matplotlib.pyplot as plt\n%matplotlib inline", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"ref2\"></a>\n<h2>Loading in the Data</h2>\n\nLet's begin by loading in our data with Pandas. The .dat files containing our data are similar to CSV files, but instead of using the ',' (comma) character to separate entries, it uses '::' (two colons) characters instead. To let Pandas know that it should separate data points at every '::', we have to specify the <code>sep='::'</code> parameter when calling the function.\n\nAdditionally, we also pass it the <code>header=None</code> parameter due to the fact that our files don't contain any headers.\n\nLet's start with the movies.dat file and take a look at its structure:"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "#Loading in the movies dataset\nmovies_df = pd.read_csv('ml-1m/movies.dat', sep='::', header=None, engine='python')\nmovies_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can do the same for the ratings.dat file:"}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "#Loading in the ratings dataset\nratings_df = pd.read_csv('ml-1m/ratings.dat', sep='::', header=None, engine='python')\nratings_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "So our <b>movies_df</b> variable contains a dataframe that stores a movie's unique ID number, title and genres, while our <b>ratings_df</b> variable stores a unique User ID number, a movie's ID that the user has watched, the user's rating to said movie and when the user rated that movie.\n\nLet's now rename the columns in these dataframes so we can better convey their data more intuitively:"}, {"metadata": {}, "cell_type": "code", "source": "movies_df.columns = ['MovieID', 'Title', 'Genres']\nmovies_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "And our final ratings_df:"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "ratings_df.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']\nratings_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"ref3\"></a>\n<h2>The Restricted Boltzmann Machine model</h2>"}, {"metadata": {}, "cell_type": "markdown", "source": "<img src=\"https://github.com/fawazsiddiqi/recommendation-system-with-a-Restricted-Boltzmann-Machine-using-tensorflow/blob/master/images/films.png?raw=true\"  width=\"300\">\n<br>\nThe Restricted Boltzmann Machine model has two layers of neurons, one of which is what we call a visible input layer and the other is called a hidden layer. The hidden layer is used to learn features from the information fed through the input layer. For our model, the input is going to contain X neurons, where X is the amount of movies in our dataset. Each of these neurons will possess a normalized rating value varying from 0 to 1, where 0 meaning that a user has not watched that movie and the closer the value is to 1, the more the user likes the movie that neuron's representing. These normalized values, of course, will be extracted and normalized from the ratings dataset.\n\nAfter passing in the input, we train the RBM on it and have the hidden layer learn its features. These features are what we use to reconstruct the input, which in our case, will predict the ratings for movies that user hasn't watched, which is exactly what we can use to recommend movies!\n\nWe will now begin to format our dataset to follow the model's expected input."}, {"metadata": {}, "cell_type": "markdown", "source": "<h3>Formatting the Data</h3>"}, {"metadata": {}, "cell_type": "markdown", "source": "First let's see how many movies we have and see if the movie ID's correspond with that value:"}, {"metadata": {}, "cell_type": "code", "source": "len(movies_df)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now, we can start formatting the data into input for the RBM. We're going to store the normalized users ratings into as a matrix of user-rating called trX, and normalize the values."}, {"metadata": {}, "cell_type": "code", "source": "user_rating_df = ratings_df.pivot(index='UserID', columns='MovieID', values='Rating')\nuser_rating_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Lets normalize it now:"}, {"metadata": {}, "cell_type": "code", "source": "norm_user_rating_df = user_rating_df.fillna(0) / 5.0\ntrX = norm_user_rating_df.values\ntrX[0:5]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"ref4\"></a>\n<h2>Setting the Model's Parameters</h2>"}, {"metadata": {}, "cell_type": "markdown", "source": "Next, let's start building our RBM with TensorFlow. We'll begin by first determining the number of neurons in the hidden layers and then creating placeholder variables for storing our visible layer biases, hidden layer biases and weights that connects the hidden layer with the visible layer. We will be arbitrarily setting the number of neurons in the hidden layers to 20. You can freely set this value to any number you want since each neuron in the hidden layer will end up learning a feature."}, {"metadata": {}, "cell_type": "code", "source": "hiddenUnits = 20\nvisibleUnits =  len(user_rating_df.columns)\n\nvb = tf.Variable(tf.zeros([visibleUnits]), tf.float32) #Number of unique movies\nhb = tf.Variable(tf.zeros([hiddenUnits]), tf.float32) #Number of features we're going to learn\nW = tf.Variable(tf.zeros([visibleUnits, hiddenUnits]), tf.float32)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We then move on to creating the visible and hidden layer units and setting their activation functions. In this case, we will be using the <code>tf.sigmoid</code> and <code>tf.relu</code> functions as nonlinear activations since it is commonly used in RBM's."}, {"metadata": {}, "cell_type": "code", "source": "v0 = tf.zeros([visibleUnits], tf.float32)\n#testing to see if the matrix product works\ntf.matmul([v0], W)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Phase 1: Input Processing\n#defining a function to return only the generated hidden states \ndef hidden_layer(v0_state, W, hb):\n    h0_prob = tf.nn.sigmoid(tf.matmul([v0_state], W) + hb)  #probabilities of the hidden units\n    h0_state = tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob)))) #sample_h_given_X\n    return h0_state\n\n#printing output of zeros input\nh0 = hidden_layer(v0, W, hb)\nprint(\"first 15 hidden states: \", h0[0][0:15])\n\ndef reconstructed_output(h0_state, W, vb):\n    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb) \n    v1_state = tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob)))) #sample_v_given_h\n    return v1_state[0]\n\n\nv1 = reconstructed_output(h0, W, vb)\nprint(\"hidden state shape: \", h0.shape)\nprint(\"v0 state shape:  \", v0.shape)\nprint(\"v1 state shape:  \", v1.shape)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "And set the error function, which in this case will be the Mean Absolute Error Function."}, {"metadata": {}, "cell_type": "code", "source": "def error(v0_state, v1_state):\n    return tf.reduce_mean(tf.square(v0_state - v1_state))\n\nerr = tf.reduce_mean(tf.square(v0 - v1))\nprint(\"error\" , err.numpy())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now we train the RBM with 5 epochs with each epoch using a batchsize of 500, giving 12 batches. After training, we print out a graph with the error by epoch."}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "epochs = 5\nbatchsize = 500\nerrors = []\nweights = []\nK=1\nalpha = 0.1\n\n#creating datasets\ntrain_ds = \\\n    tf.data.Dataset.from_tensor_slices((np.float32(trX))).batch(batchsize)\n\n\n\n#for i in range(epochs):\n#    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n#        batch = trX[start:end]\n#        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n#        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n#        cur_nb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n#        prv_w = cur_w\n#        prv_vb = cur_vb\n#        prv_hb = cur_hb\n#    errors.append(sess.run(err_sum, feed_dict={v0: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))\n#    print (errors[-1])\nv0_state=v0\nfor epoch in range(epochs):\n    batch_number = 0\n    for batch_x in train_ds:\n\n        for i_sample in range(len(batch_x)):           \n            for k in range(K):\n                v0_state = batch_x[i_sample]\n                h0_state = hidden_layer(v0_state, W, hb)\n                v1_state = reconstructed_output(h0_state, W, vb)\n                h1_state = hidden_layer(v1_state, W, hb)\n\n                delta_W = tf.matmul(tf.transpose([v0_state]), h0_state) - tf.matmul(tf.transpose([v1_state]), h1_state)\n                W = W + alpha * delta_W\n\n                vb = vb + alpha * tf.reduce_mean(v0_state - v1_state, 0)\n                hb = hb + alpha * tf.reduce_mean(h0_state - h1_state, 0) \n\n                v0_state = v1_state\n\n            if i_sample == len(batch_x)-1:\n                err = error(batch_x[i_sample], v1_state)\n                errors.append(err)\n                weights.append(W)\n                print ( 'Epoch: %d' % (epoch + 1), \n                       \"batch #: %i \" % batch_number, \"of %i\" % (len(trX)/batchsize), \n                       \"sample #: %i\" % i_sample,\n                       'reconstruction error: %f' % err)\n        batch_number += 1\n\n\n\n\nplt.plot(errors)\nplt.ylabel('Error')\nplt.xlabel('Epoch')\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"ref5\"></a>\n<h2>Recommendation</h2>"}, {"metadata": {}, "cell_type": "markdown", "source": "We can now predict movies that an arbitrarily selected user might like. This can be accomplished by feeding in the user's watched movie preferences into the RBM and then reconstructing the input. The values that the RBM gives us will attempt to estimate the user's preferences for movies that he hasn't watched based on the preferences of the users that the RBM was trained on."}, {"metadata": {}, "cell_type": "markdown", "source": "Lets first select a <b>User ID</b> of our mock user:"}, {"metadata": {}, "cell_type": "code", "source": "mock_user_id = 215", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Selecting the input user\ninputUser = trX[mock_user_id-1].reshape(1, -1)\n\ninputUser = tf.convert_to_tensor(trX[mock_user_id-1],\"float32\")\nv0 = inputUser\n\nprint(v0)\nv0.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "v0test = tf.zeros([visibleUnits], tf.float32)\nv0test.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Feeding in the user and reconstructing the input\n\nhh0 = tf.nn.sigmoid(tf.matmul([v0], W) + hb)\n\nvv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n\nrec = vv1\n\ntf.maximum(rec,1)\nfor i in vv1:\n    print(i)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can then list the 20 most recommended movies for our mock user by sorting it by their scores given by our model."}, {"metadata": {}, "cell_type": "code", "source": "scored_movies_df_mock = movies_df[movies_df['MovieID'].isin(user_rating_df.columns)]\nscored_movies_df_mock = scored_movies_df_mock.assign(RecommendationScore = rec[0])\nscored_movies_df_mock.sort_values([\"RecommendationScore\"], ascending=False).head(20)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "So, how to recommend the movies that the user has not watched yet? "}, {"metadata": {}, "cell_type": "markdown", "source": "Now, we can find all the movies that our mock user has watched before:"}, {"metadata": {}, "cell_type": "code", "source": "movies_df_mock = ratings_df[ratings_df['UserID'] == mock_user_id]\nmovies_df_mock.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "In the next cell, we merge all the movies that our mock users has watched with the predicted scores based on his historical data:"}, {"metadata": {}, "cell_type": "code", "source": "#Merging movies_df with ratings_df by MovieID\nmerged_df_mock = scored_movies_df_mock.merge(movies_df_mock, on='MovieID', how='outer')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "lets sort it and take a look at the first 20 rows:"}, {"metadata": {}, "cell_type": "code", "source": "merged_df_mock.sort_values([\"RecommendationScore\"], ascending=False).head(20)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As you can see, there are some movies that user has not watched yet and has high score based on our model. So, we can recommend them to the user."}, {"metadata": {}, "cell_type": "markdown", "source": "This is the end of the tutorial. If you want, you can try to change the parameters in the code -- adding more units to the hidden layer, changing the loss functions or maybe something else to see if it changes anything. Optimization settings can also be adjusted...the number of epochs, the size of K, and the batch size are all interesting numbers to explore.\nDoes the model perform better? Does it take longer to compute?\n\nThank you for reading this notebook. Hopefully, you now have a little more understanding of the RBM model, its applications and how it works with TensorFlow."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Want to learn more?\n\nYou can use __Watson Studio__ to run these notebooks faster with bigger datasets.__Watson Studio__ is IBM\u2019s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, __Watson Studio__ enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of __Watson Studio__ users today with a free account at [Watson Studio](http://ibm.biz/WatsonStudioRBM).This is the end of this lesson. Thank you for reading this notebook, and good luck on your studies."}, {"metadata": {}, "cell_type": "markdown", "source": "### Thank you for completing this exercise!\n\nNotebook created by: <a href = \"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, Gabriel Garcez Barros Sousa\n\nUpdated to TF 2.X by  <a href=\"https://ca.linkedin.com/in/nilmeier\"> Jerome Nilmeier</a><br />\n\nAdded to IBM Developer by <a href=https://www.linkedin.com/in/fawazsiddiqi/> Mohammad Fawaz Siddiqi </a> <br/>"}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\nCopyright &copy; 2020 [Cognitive Class](https://cocl.us/DX0108EN_CC). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "widgets": {"state": {}, "version": "1.1.2"}}, "nbformat": 4, "nbformat_minor": 2}